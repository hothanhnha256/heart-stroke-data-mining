# üìä B√ÅO C√ÅO D·ª∞ √ÅN: HEART STROKE PREDICTION

**D·ª± √°n Data Mining - HK251**  
**Ng√†y b√°o c√°o**: 18/10/2025  
**Repository**: [heart-stroke-data-mining](https://github.com/hothanhnha256/heart-stroke-data-mining)

---

## üìë M·ª§C L·ª§C

1. [T·ªïng quan Dataset](#1-t·ªïng-quan-dataset)
2. [Exploratory Data Analysis (EDA)](#2-exploratory-data-analysis-eda)
3. [Preprocessing Pipeline](#3-preprocessing-pipeline)
4. [Feature Selection](#4-feature-selection)
5. [Model Training & Results](#5-model-training--results)
6. [K·∫øt lu·∫≠n v√† Ki·∫øn ngh·ªã](#6-k·∫øt-lu·∫≠n-v√†-ki·∫øn-ngh·ªã)

---

## 1. T·ªîNG QUAN DATASET

### 1.1 Ngu·ªìn d·ªØ li·ªáu

- **Dataset**: Healthcare Dataset Stroke Data (Kaggle)
- **K√≠ch th∆∞·ªõc**: 5,110 b·ªánh nh√¢n √ó 12 thu·ªôc t√≠nh
- **M·ª•c ti√™u**: D·ª± ƒëo√°n nguy c∆° ƒë·ªôt qu·ªµ (stroke) d·ª±a tr√™n c√°c y·∫øu t·ªë s·ª©c kh·ªèe v√† nh√¢n kh·∫©u h·ªçc
- **Lo·∫°i b√†i to√°n**: Binary Classification (stroke: 0/1)

- **Lo·∫°i b√†i to√°n**: Binary Classification (stroke: 0/1)

### 1.2 C·∫•u tr√∫c d·ªØ li·ªáu

| C·ªôt                 | Ki·ªÉu    | M√¥ t·∫£                  | Ph·∫°m vi/Gi√° tr·ªã                                          | Missing |
| ------------------- | ------- | ---------------------- | -------------------------------------------------------- | ------- |
| `id`                | int     | M√£ ƒë·ªãnh danh b·ªánh nh√¢n | 1-72940                                                  | 0       |
| `gender`            | object  | Gi·ªõi t√≠nh              | Male, Female, Other                                      | 0       |
| `age`               | float   | Tu·ªïi                   | 0.08-82                                                  | 0       |
| `hypertension`      | int     | TƒÉng huy·∫øt √°p          | 0, 1                                                     | 0       |
| `heart_disease`     | int     | B·ªánh tim               | 0, 1                                                     | 0       |
| `ever_married`      | object  | T√¨nh tr·∫°ng h√¥n nh√¢n    | Yes, No                                                  | 0       |
| `work_type`         | object  | Lo·∫°i c√¥ng vi·ªác         | Private, Govt_job, Self-employed, Never_worked, children | 0       |
| `Residence_type`    | object  | N∆°i c∆∞ tr√∫             | Urban, Rural                                             | 0       |
| `avg_glucose_level` | float   | M·ª©c glucose TB (mg/dL) | 55.12-271.74                                             | 0       |
| `bmi`               | float   | Ch·ªâ s·ªë kh·ªëi c∆° th·ªÉ     | 10.3-97.6                                                | **201** |
| `smoking_status`    | object  | T√¨nh tr·∫°ng h√∫t thu·ªëc   | formerly smoked, never smoked, smokes, Unknown           | 0       |
| **`stroke`**        | **int** | **Target - ƒê·ªôt qu·ªµ**   | **0, 1**                                                 | **0**   |

### 1.3 V·∫•n ƒë·ªÅ ch√≠nh c·ªßa Dataset

#### ‚ö†Ô∏è **Problem 1: CLASS IMBALANCE nghi√™m tr·ªçng**

```
No Stroke (0): 4,861 cases (95.13%)
Stroke (1):      249 cases (4.87%)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
T·ª∑ l·ªá:         19.5 : 1
```

**Impact**:

- Model c√≥ th·ªÉ "h·ªçc" c√°ch d·ª± ƒëo√°n t·∫•t c·∫£ l√† "No Stroke" v√† v·∫´n ƒë·∫°t 95% accuracy
- C·∫ßn metrics ph√π h·ª£p: F1-Score, Precision, Recall (kh√¥ng ch·ªâ Accuracy)
- C·∫ßn k·ªπ thu·∫≠t x·ª≠ l√Ω: SMOTE, class weights, stratified sampling

#### ‚ö†Ô∏è **Problem 2: Missing Values**

```
BMI: 201 gi√° tr·ªã thi·∫øu (3.93%)
```

**Impact**:

- Kh√¥ng th·ªÉ lo·∫°i b·ªè v√¨ m·∫•t 4% d·ªØ li·ªáu
- C·∫ßn imputation strategy th√≠ch h·ª£p

#### ‚ö†Ô∏è **Problem 3: Outliers**

```
BMI:
  - Median: 28.1
  - Max: 97.6 (kh√¥ng h·ª£p l√Ω y h·ªçc!)

avg_glucose_level:
  - Median: 91.9 mg/dL
  - Max: 271.7 mg/dL (c√≥ th·ªÉ h·ª£p l√Ω trong tr∆∞·ªùng h·ª£p ƒë·∫∑c bi·ªát)
```

**Impact**:

- C√≥ th·ªÉ l√†m m√©o model
- C·∫ßn outlier capping/removal

---

## 2. EXPLORATORY DATA ANALYSIS (EDA)

### 2.1 Ph√¢n t√≠ch bi·∫øn Target

**Ph√¢n ph·ªëi Stroke:**

| Class         | Count | Percentage |
| ------------- | ----- | ---------- |
| No Stroke (0) | 4,861 | 95.13%     |
| Stroke (1)    | 249   | 4.87%      |

**Visualizations**: `eda/eda_target_distribution.png`

### 2.2 Ph√¢n t√≠ch Numeric Features

#### **Age (Tu·ªïi)**

```
Count: 5,110
Mean:  43.23 years
Std:   22.61 years
Min:   0.08 years (infant)
25%:   25 years
50%:   45 years
75%:   61 years
Max:   82 years
```

**Insight**:

- Ph√¢n ph·ªëi wide range t·ª´ infant ƒë·∫øn 82 tu·ªïi
- Stroke risk tƒÉng m·∫°nh theo tu·ªïi (see age group analysis below)

#### **Average Glucose Level**

```
Count: 5,110
Mean:  106.15 mg/dL
Std:   45.28 mg/dL
Min:   55.12 mg/dL
Median: 91.89 mg/dL
Max:   271.74 mg/dL
```

**Insight**:

- Normal range: 70-100 mg/dL (fasting)
- Nhi·ªÅu cases c√≥ glucose cao (pre-diabetes/diabetes)
- C√≥ th·ªÉ l√† risk factor quan tr·ªçng

#### **BMI (Body Mass Index)**

```
Count: 4,909 (201 missing)
Mean:  28.89
Std:   7.85
Min:   10.3 (underweight severe)
Median: 28.1
Max:   97.6 (outlier!)
```

**Insight**:

- Mean BMI = 28.89 ‚Üí "Overweight" category
- Max = 97.6 l√† outlier r√µ r√†ng (c·∫ßn x·ª≠ l√Ω)

**Visualizations**: `eda/eda_numeric_analysis.png`

### 2.3 Ph√¢n t√≠ch Categorical Features

#### **Gender Distribution**

| Gender | Count | Stroke Count | Stroke Rate |
| ------ | ----- | ------------ | ----------- |
| Female | 2,994 | 141          | 4.71%       |
| Male   | 2,115 | 108          | 5.11%       |
| Other  | 1     | 0            | 0%          |

**Insight**: Stroke rate t∆∞∆°ng ƒë∆∞∆°ng gi·ªØa Male/Female

#### **Marital Status**

| Status | Count | Stroke Count | Stroke Rate |
| ------ | ----- | ------------ | ----------- |
| Yes    | 3,353 | 220          | **6.56%**   |
| No     | 1,757 | 29           | **1.65%**   |

**Insight**: ‚ö†Ô∏è **Ng∆∞·ªùi ƒë√£ k·∫øt h√¥n c√≥ stroke rate cao g·∫•p 4 l·∫ßn!**  
(C√≥ th·ªÉ do correlation v·ªõi age - ng∆∞·ªùi l·ªõn tu·ªïi th∆∞·ªùng ƒë√£ k·∫øt h√¥n)

#### **Work Type**

| Work Type     | Count | Stroke Count | Stroke Rate |
| ------------- | ----- | ------------ | ----------- |
| Self-employed | 819   | 65           | **7.94%**   |
| Private       | 2,925 | 149          | 5.09%       |
| Govt_job      | 657   | 33           | 5.02%       |
| Children      | 687   | 2            | **0.29%**   |
| Never_worked  | 22    | 0            | 0%          |

**Insight**: Self-employed c√≥ stroke rate cao nh·∫•t

#### **Residence Type**

| Type  | Count | Stroke Count | Stroke Rate |
| ----- | ----- | ------------ | ----------- |
| Urban | 2,596 | 135          | 5.20%       |
| Rural | 2,514 | 114          | 4.53%       |

**Insight**: Kh√¥ng c√≥ s·ª± kh√°c bi·ªát l·ªõn

#### **Smoking Status**

| Status          | Count | Stroke Count | Stroke Rate |
| --------------- | ----- | ------------ | ----------- |
| formerly smoked | 885   | 70           | **7.91%**   |
| smokes          | 789   | 42           | 5.32%       |
| never smoked    | 1,892 | 90           | 4.76%       |
| Unknown         | 1,544 | 47           | 3.04%       |

**Insight**: "Formerly smoked" c√≥ rate cao nh·∫•t (c√≥ th·ªÉ do age factor)

**Visualizations**: `eda/eda_categorical_analysis.png`

### 2.4 Correlation Analysis

**Top correlations v·ªõi Stroke (theo absolute value):**

| Feature               | Correlation | √ù nghƒ©a                |
| --------------------- | ----------- | ---------------------- |
| **age**               | **0.2453**  | ‚≠ê‚≠ê‚≠ê Quan tr·ªçng nh·∫•t |
| **heart_disease**     | **0.1349**  | ‚≠ê‚≠ê Quan tr·ªçng        |
| **avg_glucose_level** | **0.1319**  | ‚≠ê‚≠ê Quan tr·ªçng        |
| **hypertension**      | **0.1279**  | ‚≠ê‚≠ê Quan tr·ªçng        |
| **ever_married**      | **0.1083**  | ‚≠ê C√≥ ·∫£nh h∆∞·ªüng        |
| bmi                   | 0.0361      | ·∫¢nh h∆∞·ªüng nh·ªè          |
| work_type             | 0.0323      | ·∫¢nh h∆∞·ªüng nh·ªè          |
| smoking_status        | 0.0281      | ·∫¢nh h∆∞·ªüng nh·ªè          |
| Residence_type        | 0.0155      | G·∫ßn nh∆∞ kh√¥ng          |
| gender                | 0.0089      | G·∫ßn nh∆∞ kh√¥ng          |

**Visualizations**: `eda/eda_correlation_matrix.png`

### 2.5 Age Group Analysis

**Stroke Rate theo nh√≥m tu·ªïi:**

| Age Group | Total | Stroke Count | Stroke Rate  |
| --------- | ----- | ------------ | ------------ |
| <30       | 1,570 | 2            | **0.13%**    |
| 30-50     | 1,413 | 21           | **1.49%**    |
| 50-65     | 1,162 | 70           | **6.02%**    |
| **65+**   | 965   | 156          | **16.17%** ÔøΩ |

**Key Insights:**

- üéØ **Age l√† predictor m·∫°nh nh·∫•t**
- Stroke rate tƒÉng **exponentially** v·ªõi tu·ªïi
- Nh√≥m 65+ c√≥ risk cao g·∫•p **127 l·∫ßn** so v·ªõi <30
- **Implication**: Age ph·∫£i l√† feature quan tr·ªçng trong model

**Visualizations**: `eda/eda_age_analysis.png`

### 2.6 T√≥m t·∫Øt EDA Insights

‚úÖ **Top Risk Factors** (theo th·ª© t·ª± quan tr·ªçng):

1. **Age** (tu·ªïi cao)
2. **Heart Disease** (b·ªánh tim)
3. **High Glucose** (ƒë∆∞·ªùng huy·∫øt cao)
4. **Hypertension** (tƒÉng huy·∫øt √°p)
5. **Marital Status** (ƒë√£ k·∫øt h√¥n - proxy cho age)

‚ùå **Weak Factors**:

- Gender (correlation g·∫ßn 0)
- Residence type (Urban vs Rural kh√¥ng kh√°c bi·ªát)

---

## 3. PREPROCESSING PIPELINE

### 3.1 Ki·∫øn tr√∫c Pipeline

Ch√∫ng t√¥i x√¢y d·ª±ng preprocessing pipeline v·ªõi **sklearn** s·ª≠ d·ª•ng `ColumnTransformer`:

```python
# Feature categorization
target_col = "stroke"
drop_cols = ["id"]  # Kh√¥ng c√≥ gi√° tr·ªã d·ª± ƒëo√°n

numeric_cols = ["age", "avg_glucose_level", "bmi"]
binary_cols = ["hypertension", "heart_disease"]
categorical_cols = ["gender", "ever_married", "work_type",
                   "Residence_type", "smoking_status"]
```

**Pipeline Flow:**

```
Raw CSV (12 columns)
    ‚Üì
[1] Missing Value Handling
    ‚Üì
[2] Outlier Capping (optional)
    ‚Üì
[3] Train/Test Split (stratified)
    ‚Üì
[4] Feature Encoding & Scaling
    ‚Üì
[5] SMOTE Balancing (train only)
    ‚Üì
Processed Data (21 features)
```

### 3.2 Chi ti·∫øt t·ª´ng b∆∞·ªõc

#### **B∆∞·ªõc 1: Missing Value Handling**

**Problem**: BMI c√≥ 201 gi√° tr·ªã thi·∫øu (3.93%)

**Solution**:

```python
# Quick imputation tr∆∞·ªõc khi outlier capping
if df["bmi"].isna().any():
    df["bmi"] = df["bmi"].fillna(df["bmi"].median())

# Safety imputer trong pipeline
SimpleImputer(strategy="median")  # Cho numeric
SimpleImputer(strategy="most_frequent")  # Cho categorical
```

**Rationale**:

- Median robust h∆°n mean v·ªõi outliers
- Most frequent gi·ªØ ƒë∆∞·ª£c distribution c·ªßa categorical

#### **B∆∞·ªõc 2: Outlier Treatment**

**Method**: IQR-based capping

```python
def cap_outliers_iqr(s, whisker=1.5):
    Q1 = s.quantile(0.25)
    Q3 = s.quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    return s.clip(lower=lower, upper=upper)
```

**Applied to**:

- `bmi`: Max 97.6 ‚Üí capped to ~45
- `avg_glucose_level`: Extreme values capped

**Rationale**:

- Kh√¥ng lo·∫°i b·ªè data points
- Gi·ªØ ƒë∆∞·ª£c information nh∆∞ng gi·∫£m impact c·ªßa outliers
- Standard statistical method

#### **B∆∞·ªõc 3: Train/Test Split**

```python
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,      # 80-20 split
    stratify=y,         # Preserve 4.87% stroke rate
    random_state=42     # Reproducibility
)
```

**Results**:

- Training: 4,088 samples
- Test: 1,022 samples
- Both maintain ~4.87% stroke rate

**Rationale**: Stratification critical v·ªõi imbalanced data

#### **B∆∞·ªõc 4: Feature Encoding & Scaling**

**A. Numeric Pipeline**:

```python
Pipeline([
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler())  # Mean=0, Std=1
])
```

**B. Categorical Pipeline**:

```python
Pipeline([
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("onehot", OneHotEncoder(
        handle_unknown="ignore",  # Quan tr·ªçng cho production!
        sparse_output=False
    ))
])
```

**C. Binary Pipeline**:

```python
"passthrough"  # Gi·ªØ nguy√™n 0/1
```

**Combined Transformer**:

```python
ColumnTransformer([
    ("num", numeric_pipeline, numeric_cols),
    ("cat", categorical_pipeline, categorical_cols),
    ("bin", "passthrough", binary_cols)
], remainder="drop")
```

**Feature Transformation**:

| Before         | After                                          | Example        |
| -------------- | ---------------------------------------------- | -------------- |
| 12 columns     | 21 features                                    | -              |
| `gender`       | `gender_Female`, `gender_Male`, `gender_Other` | OneHot         |
| `age`          | `age` (scaled)                                 | StandardScaler |
| `hypertension` | `hypertension`                                 | Passthrough    |

**‚ö†Ô∏è Critical**: Fit ch·ªâ tr√™n training set!

```python
# ‚úÖ CORRECT
preprocessor.fit(X_train)  # Learn t·ª´ train only
X_train_t = preprocessor.transform(X_train)
X_test_t = preprocessor.transform(X_test)  # Apply same transform

# ‚ùå WRONG - Data Leakage!
preprocessor.fit(X)  # Information leak t·ª´ test‚Üítrain
```

#### **B∆∞·ªõc 5: SMOTE Oversampling**

**Problem**: Training set c√≥ 4.87% stroke (197 positive / 3,891 negative)

**Solution**: SMOTE (Synthetic Minority Oversampling Technique)

```python
from imblearn.over_sampling import SMOTE

sm = SMOTE(random_state=42)
X_train_res, y_train_res = sm.fit_resample(X_train, y_train)
```

**Results**:

| Metric        | Before SMOTE   | After SMOTE     |
| ------------- | -------------- | --------------- |
| Total samples | 4,088          | **7,778**       |
| Stroke (1)    | 197 (4.82%)    | **3,889 (50%)** |
| No Stroke (0) | 3,891 (95.18%) | **3,889 (50%)** |

**How SMOTE works**:

1. L·∫•y minority class samples
2. T√¨m k-nearest neighbors
3. T·∫°o synthetic samples gi·ªØa sample v√† neighbors
4. Balance classes

**‚ö†Ô∏è Critical**: Ch·ªâ apply tr√™n TRAINING set!

```python
# ‚úÖ CORRECT
sm.fit_resample(X_train, y_train)  # Train only
# Test set gi·ªØ nguy√™n distribution

# ‚ùå WRONG
sm.fit_resample(X, y)  # Before split = data leakage!
```

### 3.3 Output Artifacts

Pipeline t·∫°o ra c√°c files trong `data-pre/`:

```
data-pre/
‚îú‚îÄ‚îÄ train_preprocessed.csv        # 7,778 √ó 22 (21 features + target)
‚îú‚îÄ‚îÄ test_preprocessed.csv         # 1,022 √ó 22
‚îú‚îÄ‚îÄ preprocessor.joblib           # Fitted sklearn pipeline
‚îú‚îÄ‚îÄ feature_names.txt             # List of 21 feature names
‚îî‚îÄ‚îÄ prep_meta.json               # Metadata
```

**prep_meta.json example**:

```json
{
  "n_train": 7778,
  "n_test": 1022,
  "pos_rate_train": 0.5, // Balanced!
  "pos_rate_test": 0.0487, // Original distribution
  "n_features": 21,
  "scale": "standard",
  "cap_outliers": true,
  "smote": true
}
```

### 3.4 Command Usage

```powershell
python prepare-stroke.py `
  --input data-raw/healthcare-dataset-stroke-data.csv `
  --output-dir data-pre `
  --test-size 0.2 `
  --scale standard `
  --cap-outliers `
  --smote `
  --random-state 42
```

**Execution time**: ~5 seconds

---

## 4. FEATURE SELECTION

### 4.1 Methodology

Ch√∫ng t√¥i s·ª≠ d·ª•ng **4 ph∆∞∆°ng ph√°p ƒë·ªôc l·∫≠p** v√† k·∫øt h·ª£p k·∫øt qu·∫£:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  1. Correlation     ‚îÇ ‚Üí Pearson correlation
‚îÇ  2. Mutual Info     ‚îÇ ‚Üí Information Gain
‚îÇ  3. RF Importance   ‚îÇ ‚Üí Tree-based
‚îÇ  4. Statistical     ‚îÇ ‚Üí ANOVA + Chi-square
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚Üì
    Normalize [0,1]
          ‚Üì
   Average scores
          ‚Üì
   Combined Ranking
```

### 4.2 Method Details

#### **Method 1: Correlation Analysis**

```python
correlations = df.corr()['stroke'].abs()
```

**Top 5 results**:

| Feature           | Correlation | Interpretation  |
| ----------------- | ----------- | --------------- |
| age               | 0.2453      | Strong positive |
| heart_disease     | 0.1349      | Moderate        |
| avg_glucose_level | 0.1319      | Moderate        |
| hypertension      | 0.1279      | Moderate        |
| ever_married      | 0.1083      | Weak-Moderate   |

**Visualization**: `feature/feature_correlation_analysis.png`

#### **Method 2: Mutual Information**

```python
from sklearn.feature_selection import mutual_info_classif
mi_scores = mutual_info_classif(X, y, random_state=42)
```

**Top 5 results**:

| Feature      | MI Score | Interpretation    |
| ------------ | -------- | ----------------- |
| age          | 0.0348   | Highest info gain |
| bmi          | 0.0112   | Moderate          |
| ever_married | 0.0093   | Moderate          |
| hypertension | 0.0091   | Moderate          |
| work_type    | 0.0074   | Low               |

**Insight**: BMI ranks higher in MI than correlation

**Visualization**: `feature/feature_mutual_info_analysis.png`

#### **Method 3: Random Forest Importance**

```python
rf = RandomForestClassifier(n_estimators=100, random_state=42,
                            class_weight='balanced')
rf.fit(X, y)
importance = rf.feature_importances_
```

**Top 5 results**:

| Feature           | Importance | Interpretation |
| ----------------- | ---------- | -------------- |
| age               | 0.3840     | Dominant!      |
| avg_glucose_level | 0.2027     | Important      |
| bmi               | 0.1844     | Important      |
| smoking_status    | 0.0482     | Minor          |
| work_type         | 0.0477     | Minor          |

**Insight**: RF heavily weights age (38.4% c·ªßa total importance!)

**Visualization**: `feature/feature_rf_importance_analysis.png`

#### **Method 4: Statistical Tests**

**For Numeric** (ANOVA F-test):

```python
from sklearn.feature_selection import f_classif
f_scores, p_values = f_classif(X_numeric, y)
```

| Feature           | F-score | p-value        |
| ----------------- | ------- | -------------- |
| age               | 326.92  | < 0.001 \*\*\* |
| avg_glucose_level | 90.50   | < 0.001 \*\*\* |
| bmi               | 6.67    | 0.0098 \*\*    |

**For Categorical** (Chi-square):

```python
from sklearn.feature_selection import chi2
chi2_scores, p_values = chi2(X_categorical, y)
```

| Feature        | œá¬≤ score | p-value        |
| -------------- | -------- | -------------- |
| heart_disease  | 87.99    | < 0.001 \*\*\* |
| hypertension   | 75.45    | < 0.001 \*\*\* |
| ever_married   | 20.62    | < 0.001 \*\*\* |
| smoking_status | 3.37     | 0.0664         |
| work_type      | 2.93     | 0.0872         |

**Visualization**: `feature/feature_statistical_analysis.png`

### 4.3 Combined Ranking

**Normalization Process**:

```python
# Min-Max normalize each method's scores to [0,1]
normalized = (score - min) / (max - min)

# Average across 4 methods
combined_score = mean([corr, mi, rf_imp, stat])
```

**Final Top 8 Features**:

| Rank | Feature               | Combined Score | Comment                 |
| ---- | --------------------- | -------------- | ----------------------- |
| ü•á 1 | **age**               | **1.0000**     | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê CRITICAL     |
| ü•à 2 | **avg_glucose_level** | **0.3636**     | ‚≠ê‚≠ê‚≠ê‚≠ê Very Important |
| ü•â 3 | **hypertension**      | **0.2471**     | ‚≠ê‚≠ê‚≠ê Important        |
| 4    | **heart_disease**     | **0.2428**     | ‚≠ê‚≠ê‚≠ê Important        |
| 5    | **bmi**               | **0.2198**     | ‚≠ê‚≠ê‚≠ê Important        |
| 6    | **ever_married**      | **0.1905**     | ‚≠ê‚≠ê Moderate           |
| 7    | **work_type**         | **0.0898**     | ‚≠ê Minor                |
| 8    | **smoking_status**    | **0.0505**     | ‚≠ê Minor                |

**Dropped** (low scores):

- `Residence_type` (0.0239)
- `gender` (0.0009)

**Visualization**: `feature/feature_combined_ranking.png`

### 4.4 Key Insights

‚úÖ **Age dominates**:

- Rank #1 in ALL 4 methods
- 10x more important than next feature
- Should definitely be included

‚úÖ **Health metrics critical**:

- glucose, hypertension, heart_disease all rank high
- Reflects medical knowledge (expected)

‚úÖ **BMI underrated by correlation**:

- Low Pearson correlation (0.036)
- But high in MI and RF importance
- **Non-linear relationship** with stroke!

‚ùå **Lifestyle factors weak**:

- Smoking: Lower than expected
- Work type: Minimal impact
- Possibly confounded by age

‚ùå **Demographic factors irrelevant**:

- Gender: Nearly zero importance
- Residence: Urban vs Rural kh√¥ng kh√°c bi·ªát

### 4.5 Recommendations

**For Modeling**:

1. **Must include**: age, avg_glucose_level, hypertension, heart_disease, bmi
2. **Consider**: ever_married (age proxy)
3. **Optional**: work_type, smoking_status
4. **Can drop**: gender, Residence_type

**Feature Engineering Ideas**:

- Age groups/bins (categorical)
- BMI categories (underweight/normal/overweight/obese)
- Glucose categories (normal/pre-diabetes/diabetes)
- Interaction features: age √ó heart_disease, age √ó hypertension

---

## 5. MODEL TRAINING & RESULTS

- **ƒê·∫∑c bi·ªát**: BMI ƒë∆∞·ª£c impute tr∆∞·ªõc ƒë·ªÉ c√≥ th·ªÉ x·ª≠ l√Ω outliers

#### 3.2.2 X·ª≠ l√Ω Outliers

- **Ph∆∞∆°ng ph√°p**: IQR-based capping v·ªõi `whisker=1.5`
- **√Åp d·ª•ng cho**: `bmi` v√† `avg_glucose_level`
- **C√¥ng th·ª©c**: `[Q1 - 1.5√óIQR, Q3 + 1.5√óIQR]`

```python
def cap_outliers_iqr(s: pd.Series, whisker: float = 1.5) -> pd.Series:
    q1, q3 = s.quantile([0.25, 0.75])
    iqr = q3 - q1
    return s.clip(lower=q1 - whisker*iqr, upper=q3 + whisker*iqr)
```

#### 3.2.3 Encoding Categorical Variables

- **Ph∆∞∆°ng ph√°p**: `OneHotEncoder(handle_unknown='ignore')`
- **K·∫øt qu·∫£**: T·ª´ 5 categorical columns ‚Üí 14 encoded features
- **V√≠ d·ª•**: `gender` ‚Üí `gender_Female`, `gender_Male`, `gender_Other`

#### 3.2.4 Feature Scaling

- **Options**: StandardScaler (default), MinMaxScaler, ho·∫∑c kh√¥ng scale
- **√Åp d·ª•ng cho**: Ch·ªâ numeric features
- **L√Ω do**: Binary features gi·ªØ nguy√™n (0/1)

#### 3.2.5 Train/Test Split

- **Ph∆∞∆°ng ph√°p**: `train_test_split` v·ªõi `stratify=y`
- **T·ª∑ l·ªá**: 80/20 (c√≥ th·ªÉ c·∫•u h√¨nh)
- **Random state**: 42 (reproducibility)

#### 3.2.6 X·ª≠ l√Ω Class Imbalance

- **Ph∆∞∆°ng ph√°p**: SMOTE (Synthetic Minority Oversampling Technique)
- **Th∆∞ vi·ªán**: `imbalanced-learn`
- **√Åp d·ª•ng**: Ch·ªâ tr√™n training set
- **K·∫øt qu·∫£**: C√¢n b·∫±ng t·ª∑ l·ªá 50/50

### 3.3 Artifacts ƒë∆∞·ª£c t·∫°o

1. `preprocessor.joblib`: Sklearn pipeline ƒë√£ fit
2. `train_preprocessed.csv`, `test_preprocessed.csv`: D·ªØ li·ªáu ƒë√£ transform
3. `feature_names.txt`: Danh s√°ch features sau encoding
4. `prep_meta.json`: Metadata v√† th·ªëng k√™

**üìã Metadata v√≠ d·ª•:**

```json
{
  "n_train": 7778, // Sau SMOTE
  "n_test": 1022,
  "pos_rate_train": 0.5, // C√¢n b·∫±ng sau SMOTE
  "pos_rate_test": 0.049, // Gi·ªØ nguy√™n ph√¢n ph·ªëi g·ªëc
  "n_features": 21
}
```

---

## 4. FEATURE SELECTION

### 4.1 Ph∆∞∆°ng ph√°p √°p d·ª•ng

Ch√∫ng t√¥i s·ª≠ d·ª•ng **4 ph∆∞∆°ng ph√°p** k·∫øt h·ª£p:

1. **Correlation Analysis**: T∆∞∆°ng quan Pearson v·ªõi target
2. **Mutual Information**: Information gain gi·ªØa features v√† target
3. **Random Forest Importance**: Feature importance t·ª´ tree-based model
4. **Statistical Tests**: ANOVA F-test (numeric) + Chi-square (categorical)

### 4.2 Quy tr√¨nh feature selection

```python
# 1. Normalize t·∫•t c·∫£ scores v·ªÅ [0,1]
# 2. T√≠nh combined_score = average c·ªßa 4 ph∆∞∆°ng ph√°p
# 3. Rank features theo combined_score
# 4. Ch·ªçn top K features
```

### 4.3 K·∫øt qu·∫£ feature selection

**üèÜ Top 8 Features quan tr·ªçng nh·∫•t:**

1. **age**: Y·∫øu t·ªë quan tr·ªçng nh·∫•t (tu·ªïi)
2. **avg_glucose_level**: M·ª©c glucose trung b√¨nh
3. **bmi**: Ch·ªâ s·ªë kh·ªëi c∆° th·ªÉ
4. **hypertension**: TƒÉng huy·∫øt √°p
5. **heart_disease**: B·ªánh tim
6. **work*type*\***: M·ªôt s·ªë lo·∫°i c√¥ng vi·ªác c·ª• th·ªÉ
7. **ever*married*\***: T√¨nh tr·∫°ng h√¥n nh√¢n
8. **smoking*status*\***: T√¨nh tr·∫°ng h√∫t thu·ªëc

**üìä K·∫øt qu·∫£ chi ti·∫øt**: Xem `feature_selection.py` v√† `feature_selection_results.json`

### 4.4 Insights t·ª´ Feature Selection

- **Age dominates**: Tu·ªïi l√† predictor m·∫°nh nh·∫•t
- **Health indicators**: C√°c ch·ªâ s·ªë s·ª©c kh·ªèe (glucose, BMI, blood pressure) quan tr·ªçng
- **Lifestyle factors**: H√∫t thu·ªëc, h√¥n nh√¢n c√≥ ·∫£nh h∆∞·ªüng nh∆∞ng √≠t h∆°n
- **Gender**: Kh√¥ng n·∫±m trong top features

---

## 5. MODEL TRAINING & RESULTS

### 5.1 Models Overview

Ch√∫ng t√¥i ƒë√£ tri·ªÉn khai v√† so s√°nh **4 models** kh√°c nhau:

1. **Logistic Regression** - Baseline linear model
2. **Random Forest** - Ensemble tree-based model
3. **SVM (RBF kernel)** - Support Vector Machine v·ªõi kernel phi tuy·∫øn
4. **K-Nearest Neighbors (k=5)** - Instance-based learning

**Training Data**: 7,778 samples (balanced 50-50 sau SMOTE)  
**Test Data**: 1,022 samples (original distribution: 95.1% vs 4.9%)

### 5.2 Results Summary

#### **üìä Performance Metrics Table**

| Model                      | Accuracy   | Precision  | Recall     | F1-Score   | ROC-AUC    |
| -------------------------- | ---------- | ---------- | ---------- | ---------- | ---------- |
| **Logistic Regression** ‚≠ê | **0.7495** | **0.1399** | **0.8000** | **0.2381** | **0.8456** |
| Random Forest              | 0.9266     | 0.1795     | 0.1400     | 0.1573     | 0.7615     |
| SVM (RBF)                  | 0.7945     | 0.1040     | 0.4200     | 0.1667     | 0.7648     |
| KNN (k=5)                  | 0.8415     | 0.0942     | 0.2600     | 0.1383     | 0.6202     |

**Visualization**: `model_metrics_comparison.png`

#### **üèÜ Rankings**

**Theo F1-Score** (ch√≠nh):

1. ü•á **Logistic Regression**: 0.2381
2. ü•à SVM (RBF): 0.1667
3. ü•â Random Forest: 0.1573
4. KNN (k=5): 0.1383

**Theo ROC-AUC**:

1. ü•á **Logistic Regression**: 0.8456
2. ü•à SVM (RBF): 0.7648
3. ü•â Random Forest: 0.7615
4. KNN (k=5): 0.6202

**Theo Recall** (quan tr·ªçng cho medical):

1. ü•á **Logistic Regression**: 0.8000 (40/50 cases detected)
2. ü•à SVM (RBF): 0.4200 (21/50)
3. ü•â KNN (k=5): 0.2600 (13/50)
4. Random Forest: 0.1400 (7/50) ‚ö†Ô∏è

### 5.3 Detailed Analysis

#### **Model 1: Logistic Regression** ‚≠ê BEST

**Confusion Matrix**:

```
              Predicted
              No    Yes
Actual No    726    246
       Yes    10     40
```

**Strengths**:

- ‚úÖ **Best Recall (0.80)**: Detected 40/50 stroke cases ‚Üí ch·ªâ miss 10
- ‚úÖ **Best ROC-AUC (0.8456)**: Excellent discrimination ability
- ‚úÖ **Best F1-Score (0.2381)**: Best balance precision-recall
- ‚úÖ **Low False Negatives (10)**: Critical cho medical context

**Weaknesses**:

- ‚ùå **Low Precision (0.14)**: 246 false alarms
- ‚ùå **High False Positives**: Nhi·ªÅu ng∆∞·ªùi kh√¥ng c√≥ stroke b·ªã d·ª± ƒëo√°n nh·∫ßm

**Medical Interpretation**:

- **Sensitivity**: 80% strokes detected ‚Üí Excellent for screening
- **Trade-off**: "Over-alert" model ‚Üí better safe than sorry

#### **Model 2: Random Forest**

**Confusion Matrix**:

```
              Predicted
              No    Yes
Actual No    940     32
       Yes    43      7
```

**Strengths**:

- ‚úÖ **Highest Accuracy (0.93)**: Best overall correctness
- ‚úÖ **Low False Positives (32)**: Fewest false alarms

**Weaknesses**:

- ‚ùå **Worst Recall (0.14)**: Ch·ªâ detect 7/50 ‚Üí **MISS 43 cases!** ‚ö†Ô∏è
- ‚ùå **Not suitable for medical**: Too many missed strokes (86%)

#### **Model 3: SVM (RBF)**

**Confusion Matrix**:

```
              Predicted
              No    Yes
Actual No    791    181
       Yes    29     21
```

**Analysis**:

- Moderate performance across all metrics
- Better than RF v·ªÅ recall nh∆∞ng worse than LogReg
- Computationally expensive without significant improvement

#### **Model 4: KNN (k=5)**

**Confusion Matrix**:

```
              Predicted
              No    Yes
Actual No    847    125
       Yes    37     13
```

**Analysis**:

- Worst ROC-AUC (0.62) ‚Üí Poor discrimination
- 74% strokes missed (37/50)
- Not recommended for production

### 5.4 Model Selection

#### **üèÜ Recommended: Logistic Regression**

**L√Ω do ch·ªçn**:

1. **Best F1-Score (0.2381)** - Highest overall performance
2. **Best Recall (0.80)** - Critical cho medical diagnosis
3. **Best ROC-AUC (0.8456)** - Best discrimination ability
4. **Interpretable** - Linear coefficients hi·ªÉu ƒë∆∞·ª£c
5. **Fast** - Training + prediction r·∫•t nhanh

**Production Strategy**:

```python
# Option 1: High Sensitivity (Current)
threshold = 0.5
recall = 0.80  # 80% detection
precision = 0.14  # Acceptable false alarms

# Option 2: Adjusted Threshold
threshold = 0.3  # Lower threshold
expected_recall = 0.90+  # 90%+ detection
expected_precision = 0.08-0.10  # More false alarms
```

**Medical Justification**:

- False Positives ‚Üí Extra tests (acceptable cost)
- False Negatives ‚Üí Missed diagnosis (unacceptable!)
- **Better safe than sorry**

### 5.5 Evaluation Metrics Explanation

**Why F1-Score?**:

- Harmonic mean c·ªßa Precision & Recall
- Balanced metric cho imbalanced data
- Penalizes extreme imbalance

**Why ROC-AUC?**:

- Measures discrimination across all thresholds
- Robust to class imbalance
- 0.8456 = "Good" classification

**Why NOT Accuracy?**:

- 95% No Stroke ‚Üí predict t·∫•t c·∫£ "No Stroke" = 95% accuracy
- Ho√†n to√†n v√¥ d·ª•ng!
- Misleading metric cho imbalanced data

---

## 6. K·∫æT LU·∫¨N V√Ä KI·∫æN NGH·ªä

### 6.1 T√≥m t·∫Øt Th√†nh qu·∫£

‚úÖ **Dataset Understanding**:

- Analyzed 5,110 patients v·ªõi class imbalance nghi√™m tr·ªçng (95.1% vs 4.9%)
- X√°c ƒë·ªãnh age l√† predictor m·∫°nh nh·∫•t (correlation 0.2453)
- Ph√°t hi·ªán BMI c√≥ non-linear relationship v·ªõi stroke
- Age group 65+ c√≥ risk cao g·∫•p 127 l·∫ßn so v·ªõi <30

‚úÖ **Preprocessing Pipeline**:

- X·ª≠ l√Ω missing values: Median imputation cho BMI (201 missing)
- Outlier capping: IQR method cho BMI v√† glucose
- Feature encoding: 12 columns ‚Üí 21 features
- SMOTE balancing: Train set balanced 50-50 (7,778 samples)
- **Zero data leakage**: Fit preprocessor on train only

‚úÖ **Feature Selection**:

- Multi-method approach: 4 independent methods
- Top 8 features identified: age, glucose, hypertension, heart_disease, bmi, ever_married, work_type, smoking
- Validates medical knowledge: Age v√† health metrics dominate
- Gender v√† Residence_type c√≥ th·ªÉ drop (minimal impact)

‚úÖ **Model Training**:

- Implemented 4 models: LogReg, Random Forest, SVM, KNN
- Best model: **Logistic Regression**
  - F1-Score: 0.2381 (best)
  - Recall: 0.80 (80% stroke detection)
  - ROC-AUC: 0.8456 (excellent discrimination)
- Model comparison v·ªõi comprehensive visualizations

‚úÖ **Production-Ready**:

- Complete pipeline t·ª´ raw data ‚Üí predictions
- Reproducible v·ªõi random_state=42
- Documented code v·ªõi Vietnamese comments
- Scripts: `run_all_models.py` cho full comparison

### 6.2 Challenges & Solutions

#### **Challenge 1: Class Imbalance (95:5)**

**Impact**:

- Models learn biased towards majority class
- High accuracy but poor minority class detection
- Random Forest achieved 93% accuracy nh∆∞ng ch·ªâ detect 14% strokes!

**Solutions Implemented**:
‚úÖ SMOTE oversampling tr√™n training set  
‚úÖ Stratified train/test split  
‚úÖ Focus on F1-Score v√† Recall thay v√¨ Accuracy  
‚úÖ Logistic Regression with class_weight='balanced'

**Results**:

- Training balanced 50-50
- Test gi·ªØ original distribution (realistic evaluation)
- Recall improved to 80% v·ªõi LogReg

#### **Challenge 2: Missing Values (BMI)**

**Impact**: 201/5110 (3.93%) missing BMI values

**Solution**:
‚úÖ Median imputation (robust to outliers)  
‚úÖ Impute BEFORE outlier capping  
‚úÖ Preserve distribution

**Results**: Zero missing values after preprocessing

#### **Challenge 3: Feature Complexity**

**Impact**: Mixed data types (numeric, binary, categorical)

**Solution**:
‚úÖ `ColumnTransformer` v·ªõi separate pipelines  
‚úÖ Numeric: Imputer ‚Üí Scaler  
‚úÖ Categorical: Imputer ‚Üí OneHotEncoder  
‚úÖ Binary: Passthrough

**Results**: Clean 21-feature matrix

#### **Challenge 4: Model Selection**

**Impact**: Trade-off gi·ªØa precision v√† recall

**Analysis**:

- Random Forest: 93% accuracy nh∆∞ng 86% missed strokes ‚ùå
- Logistic Regression: 75% accuracy nh∆∞ng 80% stroke detection ‚úÖ

**Decision**:
‚úÖ Choose Logistic Regression  
‚úÖ Prioritize Recall cho medical context  
‚úÖ Accept false positives for safety

### 6.3 Key Insights

#### **Medical Insights**

üè• **Age is dominant predictor**:

- 10x more important than any other feature
- Age 65+ has 16.17% stroke rate vs 0.13% for <30
- Non-negotiable feature for any model

üè• **Health metrics critical**:

- Glucose level: 2nd most important (correlation 0.132)
- Hypertension & Heart disease: Strong indicators
- BMI: Non-linear relationship (important in RF)

üè• **Lifestyle factors surprising**:

- Smoking status: Lower impact than expected
- Possibly confounded by age (elderly people quit)
- "Formerly smoked" c√≥ highest rate (7.91%) ‚Üí age effect

üè• **Demographics less important**:

- Gender: Nearly zero importance
- Urban vs Rural: No significant difference
- Work type: Minimal impact

#### **Machine Learning Insights**

ü§ñ **Simpler is better**:

- Logistic Regression outperforms complex models
- Random Forest overfits to majority class
- SVM computationally expensive without gains

ü§ñ **SMOTE effectiveness**:

- Balanced training crucial for minority class learning
- Must apply AFTER train/test split
- Don't apply to test set (realistic evaluation)

ü§ñ **Metrics matter**:

- Accuracy is misleading (95% baseline)
- F1-Score balances precision-recall
- Recall prioritized for medical screening

ü§ñ **Threshold tuning potential**:

- Default 0.5 gives 80% recall
- Lowering to 0.3 could achieve 90%+ recall
- Trade-off: More false positives (acceptable)

### 6.4 Limitations

‚ö†Ô∏è **Data Limitations**:

- Dataset size: 5,110 samples (moderate)
- Temporal coverage: Single timepoint (no longitudinal)
- Missing BMI: 3.93% could introduce bias
- "Unknown" smoking status: 30% unclear classification

‚ö†Ô∏è **Model Limitations**:

- Low precision (0.14): High false alarm rate
- F1-Score 0.24: Room for improvement
- No feature interactions explored
- No hyperparameter tuning (Random Forest, SVM)

‚ö†Ô∏è **Generalization Concerns**:

- Dataset from single source (Kaggle)
- Population may not represent all demographics
- Geographic bias unknown
- Temporal validity unclear (year of data collection)

### 6.5 Future Improvements

#### **Short-term** (Immediate)

1. **Hyperparameter Tuning**:

   ```python
   GridSearchCV ho·∫∑c RandomizedSearchCV
   - Logistic Regression: C, penalty
   - Random Forest: n_estimators, max_depth, min_samples_split
   - SVM: C, gamma
   ```

2. **Threshold Optimization**:

   ```python
   # Find optimal threshold maximizing F1 or Recall
   from sklearn.metrics import precision_recall_curve
   precision, recall, thresholds = precision_recall_curve(y_test, y_prob)
   ```

3. **Feature Engineering**:
   - Age bins: <30, 30-50, 50-65, 65+
   - BMI categories: Underweight, Normal, Overweight, Obese
   - Glucose categories: Normal, Pre-diabetes, Diabetes
   - Interaction features: age √ó heart_disease

#### **Medium-term**

4. **Ensemble Methods**:

   ```python
   # Voting Classifier
   VotingClassifier([
       ('lr', LogisticRegression()),
       ('svm', SVC(probability=True))
   ], voting='soft')
   ```

5. **Cross-Validation**:

   ```python
   StratifiedKFold(n_splits=5)
   # More robust performance estimates
   ```

6. **Cost-Sensitive Learning**:

   ```python
   # Assign higher cost to False Negatives
   class_weight = {0: 1, 1: 10}
   ```

7. **SHAP Analysis**:
   ```python
   import shap
   # Explain individual predictions
   # Feature importance v·ªõi interactions
   ```

#### **Long-term** (Research)

8. **Deep Learning**:

   - Neural Networks cho complex patterns
   - Autoencoders cho anomaly detection
   - Caution: Needs more data (5K may be insufficient)

9. **Longitudinal Data**:

   - Track patients over time
   - Survival analysis
   - Time-to-stroke prediction

10. **External Validation**:

    - Test on different datasets
    - Multi-site validation
    - Cross-population generalization

11. **Clinical Integration**:
    - Risk calculator web app
    - Integration with EMR systems
    - Real-time prediction API

### 6.6 Production Deployment Recommendations

#### **Deployment Architecture**

```
User Input ‚Üí Preprocessing Pipeline ‚Üí Model ‚Üí Risk Score ‚Üí Clinical Decision Support
              (preprocessor.joblib)   (LogReg)   (0-1)
```

**Steps**:

1. **Input Validation**: Check all required features present
2. **Preprocessing**: Apply saved `preprocessor.joblib`
3. **Prediction**: LogisticRegression.predict_proba()
4. **Interpretation**:
   - probability > 0.5 ‚Üí High Risk
   - 0.3-0.5 ‚Üí Medium Risk
   - < 0.3 ‚Üí Low Risk
5. **Output**: Risk score + feature contributions (SHAP values)

#### **Monitoring Strategy**

**Track metrics**:

- Prediction distribution over time
- Feature drift (data distribution changes)
- Model performance on new data
- False Negative rate (critical!)

**Retrain triggers**:

- Performance degrades > 5% F1-Score drop
- Feature distribution shift detected
- New data accumulated (> 20% of original)
- Quarterly scheduled retraining

### 6.7 Final Recommendations

#### **For Clinicians**

‚úÖ **Use as screening tool**:

- High sensitivity (80%) good for initial screening
- Positive prediction ‚Üí Further diagnostic tests
- Negative prediction ‚Üí Lower risk but monitor

‚úÖ **Focus on high-risk groups**:

- Age 65+ (16% stroke rate)
- Hypertension + Heart disease patients
- High glucose levels

‚ö†Ô∏è **Limitations to communicate**:

- Not diagnostic (14% precision)
- Many false alarms expected
- Clinical judgment essential

#### **For Data Scientists**

‚úÖ **Key Lessons**:

- Class imbalance: Use SMOTE + stratified split
- Metrics: F1/Recall > Accuracy for medical
- Simple models: Often outperform complex ones
- Feature selection: Validates domain knowledge

‚úÖ **Best Practices**:

- Zero data leakage (fit on train only)
- Reproducibility (random_state, seeds)
- Documentation (Vietnamese + English)
- Visualization (comprehensive charts)

‚úÖ **Next Steps**:

1. Hyperparameter tuning
2. Ensemble methods
3. SHAP explanations
4. External validation

#### **For Stakeholders**

‚úÖ **Business Value**:

- Early stroke detection ‚Üí Better outcomes
- Cost-effective screening tool
- Scalable to large populations

‚úÖ **Risk Management**:

- High false positives ‚Üí Extra tests cost
- Low false negatives ‚Üí Missed diagnosis risk
- Current model: Conservative (better safe)

‚úÖ **Deployment Path**:

- Pilot study with clinical validation
- Integration with existing workflows
- Continuous monitoring and improvement

### 6.8 Acknowledgments

**Dataset**: Healthcare Dataset Stroke Data (Kaggle)  
**Libraries**: scikit-learn, pandas, numpy, imbalanced-learn, matplotlib, seaborn  
**Tools**: Python 3.11.4, VS Code, Git  
**Repository**: https://github.com/hothanhnha256/heart-stroke-data-mining

---

## APPENDIX

### A. Command Reference

**Complete Workflow**:

```powershell
# 1. Setup environment
python -m venv .venv
.venv\Scripts\activate
pip install -r requirements.txt

# 2. EDA
python eda_analysis.py

# 3. Preprocessing
python prepare-stroke.py `
  --input data-raw/healthcare-dataset-stroke-data.csv `
  --output-dir data-pre `
  --scale standard `
  --cap-outliers `
  --smote

# 4. Feature Selection
python feature_selection.py

# 5. Model Training & Comparison
python run_all_models.py

# 6. Individual Models
python model-A/logistics_reg.py
python model-A/random_forest.py
python model-B/svm.py
```

### B. File Structure

```
heart-stroke/
‚îú‚îÄ‚îÄ data-raw/
‚îÇ   ‚îî‚îÄ‚îÄ healthcare-dataset-stroke-data.csv    # Raw data (5,110 rows)
‚îú‚îÄ‚îÄ data-pre/
‚îÇ   ‚îú‚îÄ‚îÄ train_preprocessed.csv               # 7,778 rows (SMOTE)
‚îÇ   ‚îú‚îÄ‚îÄ test_preprocessed.csv                # 1,022 rows
‚îÇ   ‚îú‚îÄ‚îÄ preprocessor.joblib                  # Fitted pipeline
‚îÇ   ‚îú‚îÄ‚îÄ feature_names.txt                    # 21 features
‚îÇ   ‚îî‚îÄ‚îÄ prep_meta.json                       # Metadata
‚îú‚îÄ‚îÄ eda/
‚îÇ   ‚îú‚îÄ‚îÄ eda_target_distribution.png
‚îÇ   ‚îú‚îÄ‚îÄ eda_numeric_analysis.png
‚îÇ   ‚îú‚îÄ‚îÄ eda_categorical_analysis.png
‚îÇ   ‚îú‚îÄ‚îÄ eda_correlation_matrix.png
‚îÇ   ‚îî‚îÄ‚îÄ eda_age_analysis.png
‚îú‚îÄ‚îÄ feature/
‚îÇ   ‚îú‚îÄ‚îÄ feature_correlation_analysis.png
‚îÇ   ‚îú‚îÄ‚îÄ feature_mutual_info_analysis.png
‚îÇ   ‚îú‚îÄ‚îÄ feature_rf_importance_analysis.png
‚îÇ   ‚îú‚îÄ‚îÄ feature_statistical_analysis.png
‚îÇ   ‚îú‚îÄ‚îÄ feature_combined_ranking.png
‚îÇ   ‚îî‚îÄ‚îÄ feature_selection_results.json
‚îú‚îÄ‚îÄ model-A/
‚îÇ   ‚îú‚îÄ‚îÄ logistics_reg.py
‚îÇ   ‚îî‚îÄ‚îÄ random_forest.py
‚îú‚îÄ‚îÄ model-B/
‚îÇ   ‚îú‚îÄ‚îÄ svm.py
‚îÇ   ‚îî‚îÄ‚îÄ svm-and-knn.ipynb
‚îú‚îÄ‚îÄ prepare-stroke.py                        # Preprocessing pipeline
‚îú‚îÄ‚îÄ eda_analysis.py                          # EDA script
‚îú‚îÄ‚îÄ feature_selection.py                     # Feature selection
‚îú‚îÄ‚îÄ run_all_models.py                        # Model comparison
‚îú‚îÄ‚îÄ model_comparison_results.csv             # Results table
‚îú‚îÄ‚îÄ model_roc_curves_comparison.png          # ROC curves
‚îú‚îÄ‚îÄ model_metrics_comparison.png             # Metrics chart
‚îú‚îÄ‚îÄ model_confusion_matrices.png             # Confusion matrices
‚îú‚îÄ‚îÄ models_final_report.txt                  # Detailed report
‚îú‚îÄ‚îÄ models_results.json                      # JSON results
‚îú‚îÄ‚îÄ README.md                                # Documentation
‚îú‚îÄ‚îÄ REPORT.md                                # This report
‚îú‚îÄ‚îÄ QUICKSTART.md                            # Quick start guide
‚îî‚îÄ‚îÄ requirements.txt                         # Dependencies
```

### C. Key Metrics Summary

| Model                      | Accuracy | Precision | Recall     | F1-Score   | ROC-AUC    | Strokes Detected |
| -------------------------- | -------- | --------- | ---------- | ---------- | ---------- | ---------------- |
| **Logistic Regression** ‚≠ê | 0.7495   | 0.1399    | **0.8000** | **0.2381** | **0.8456** | **40/50 (80%)**  |
| Random Forest              | 0.9266   | 0.1795    | 0.1400     | 0.1573     | 0.7615     | 7/50 (14%)       |
| SVM (RBF)                  | 0.7945   | 0.1040    | 0.4200     | 0.1667     | 0.7648     | 21/50 (42%)      |
| KNN (k=5)                  | 0.8415   | 0.0942    | 0.2600     | 0.1383     | 0.6202     | 13/50 (26%)      |

**Winner**: Logistic Regression (Best F1, Recall, ROC-AUC)

---

**END OF REPORT**

**Date**: October 18, 2025  
**Project**: Heart Stroke Prediction - Data Mining HK251  
**Team**: Data Mining Project Group

- Ph√°t hi·ªán BMI c√≥ non-linear relationship

‚úÖ **Preprocessing Pipeline**:

- X·ª≠ l√Ω missing values (median/mode imputation)
- Outlier capping (IQR method)
- Feature encoding (12 cols ‚Üí 21 features)
- SMOTE balancing (c√¢n b·∫±ng 50-50 tr√™n train set)
- **Zero data leakage** (fit tr√™n train only)

‚úÖ **Feature Selection**:

- Multi-method approach (4 methods)
- Top 8 features identified
- Validates medical knowledge (age, glucose, hypertension critical)

‚úÖ **Infrastructure**:

- Modular, reusable code
- CLI interface
- Artifact management
- Team collaboration framework

### 6.2 Th√°ch th·ª©c ƒê√£ Gi·∫£i quy·∫øt

‚úÖ Class Imbalance ‚Üí SMOTE + stratification + proper metrics  
‚úÖ Missing Data ‚Üí Median/mode imputation  
‚úÖ Outliers ‚Üí IQR capping  
‚úÖ Data Leakage ‚Üí Careful pipeline design  
‚úÖ Feature Selection ‚Üí Multi-method consensus

### 6.3 H·∫°n ch·∫ø v√† ƒê·ªÅ xu·∫•t C·∫£i ti·∫øn

‚ùå **H·∫°n ch·∫ø**:

- Baseline model F1-Score th·∫•p (0.10)
- Ch∆∞a c√≥ cross-validation
- Ch∆∞a exploit non-linear relationships ƒë·∫ßy ƒë·ªß
- Ch∆∞a c√≥ ensemble methods

üîÑ **ƒê·ªÅ xu·∫•t**:

1. **Short-term**: Hyperparameter tuning, threshold optimization, cross-validation
2. **Medium-term**: Feature engineering (interactions, polynomials), ensemble (stacking, voting), XGBoost/LightGBM
3. **Long-term**: Deep learning, AutoML, production deployment

### 6.4 Lessons Learned

**Technical**:

- Accuracy misleading v·ªõi imbalanced data
- SMOTE ph·∫£i apply sau train/test split
- Feature selection c·∫ßn multiple methods
- Age >> all other features trong medical prediction

**Domain**:

- Stroke risk tƒÉng exponentially v·ªõi tu·ªïi
- Health metrics (glucose, BP, heart disease) critical
- Lifestyle factors (smoking) confounded by age
- Gender surprisingly kh√¥ng quan tr·ªçng

**Project Management**:

- Modular code ‚Üí team collaboration
- Git branches ‚Üí parallel development
- Documentation ‚Üí reduce confusion
- Automated consolidation ‚Üí save time

### 6.5 K·∫øt lu·∫≠n Cu·ªëi c√πng

D·ª± √°n ƒë√£ successfully x√¢y d·ª±ng m·ªôt **complete data mining pipeline** t·ª´ raw data ƒë·∫øn model evaluation:

‚úÖ Comprehensive EDA v·ªõi insights r√µ r√†ng  
‚úÖ Robust preprocessing preventing data leakage  
‚úÖ Scientific feature selection identifying top predictors  
‚úÖ Scalable framework cho team collaboration  
‚úÖ Production-ready code v·ªõi documentation ƒë·∫ßy ƒë·ªß

**Next Steps**: C·∫£i thi·ªán model performance qua tuning, feature engineering, v√† ensemble methods ƒë·ªÉ ƒë·∫°t clinical-grade predictions.

---

## üìö REFERENCES

1. **Dataset**: Kaggle Stroke Prediction Dataset
2. **SMOTE**: Chawla et al. - Synthetic Minority Oversampling Technique
3. **Scikit-learn**: Machine Learning in Python
4. **Imbalanced-learn**: Tools for imbalanced datasets
5. **Medical Knowledge**: WHO Stroke Guidelines, American Heart Association

---

## üìé APPENDIX

### A. Execution Commands

```powershell
# Environment Setup
python -m venv .venv
.venv\Scripts\activate
pip install -r requirements.txt

# Full Pipeline
python eda_analysis.py
python prepare-stroke.py --input data-raw/healthcare-dataset-stroke-data.csv --output-dir data-pre --scale standard --cap-outliers --smote
python feature_selection.py
python implement.py
python model_consolidation.py
```

### B. File Outputs

```
eda/*.png                          # EDA visualizations
feature/*.png                      # Feature selection charts
feature/feature_selection_results.json  # Top features ranking
data-pre/train_preprocessed.csv    # 7,778 √ó 22 (balanced)
data-pre/test_preprocessed.csv     # 1,022 √ó 22 (original dist)
data-pre/preprocessor.joblib       # Reusable pipeline
data-pre/prep_meta.json           # Metadata
model_results_comparison.png       # Model comparison charts
detailed_model_report.txt          # Detailed results
```

### C. Team Contributions

- **Data Pipeline**: Preprocessing, EDA, Feature Selection
- **Model A**: Logistic Regression, Random Forest
- **Model B**: SVM, KNN
- **Documentation**: README, REPORT, Copilot Instructions

---

**üìä End of Report**  
_Generated: 18/10/2025_  
_Project: Heart Stroke Prediction_  
_Team: HK251 Data Mining_
